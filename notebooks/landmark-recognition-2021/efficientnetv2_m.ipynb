{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06bb6ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from timm.data import resolve_model_data_config, create_transform\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from common.datasets import LandmarkDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5259e073",
   "metadata": {},
   "source": [
    "##### Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69bf7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATASET_PATH = \"D:/Datasets/landmark-recognition-2021\"\n",
    "TRAIN_DIR = f\"{DATASET_PATH}/train\"\n",
    "TEST_DIR = f\"{DATASET_PATH}/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba747168",
   "metadata": {},
   "source": [
    "##### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a76ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 16\n",
    "N_WORKERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ec305",
   "metadata": {},
   "source": [
    "##### Training IDs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a40f11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>landmark_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17660ef415d37059</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92b6290d571448f6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cd41bf948edc0340</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fb09f1e98c6d2f70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25c9dfc7ea69838d</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  landmark_id\n",
       "0  17660ef415d37059            1\n",
       "1  92b6290d571448f6            1\n",
       "2  cd41bf948edc0340            1\n",
       "3  fb09f1e98c6d2f70            1\n",
       "4  25c9dfc7ea69838d            7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(f\"{DATASET_PATH}/train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca7c871",
   "metadata": {},
   "source": [
    "##### Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7209481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0506fa83",
   "metadata": {},
   "source": [
    "##### Training/Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d159394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training IDs: 100\n",
      "Test IDs: 100\n",
      "Total labels used: 200\n"
     ]
    }
   ],
   "source": [
    "USE_SUBSET = True\n",
    "SUBSET_SIZE = 100\n",
    "\n",
    "labels = dict(zip(train_df[\"id\"], train_df[\"landmark_id\"]))\n",
    "\n",
    "unique_landmarks = sorted(train_df[\"landmark_id\"].unique())\n",
    "landmark2idx_global = {l: i for i, l in enumerate(unique_landmarks)}\n",
    "labels_global = {img_id: landmark2idx_global[l_id] for img_id, l_id in labels.items()}\n",
    "\n",
    "train_ids, test_ids = train_test_split(\n",
    "    train_df[\"id\"].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=train_df[\"landmark_id\"],\n",
    ")\n",
    "\n",
    "if USE_SUBSET:\n",
    "    train_ids_small = train_ids[:SUBSET_SIZE]\n",
    "    test_ids_small = test_ids[:SUBSET_SIZE]\n",
    "    partition = {\"train\": train_ids_small, \"test\": test_ids_small}\n",
    "\n",
    "    unique_landmarks_subset = sorted(\n",
    "        set(labels_global[img_id] for img_id in partition[\"train\"] + partition[\"test\"])\n",
    "    )\n",
    "    landmark2idx_subset = {l: i for i, l in enumerate(unique_landmarks_subset)}\n",
    "    labels_subset = {\n",
    "        img_id: landmark2idx_subset[labels_global[img_id]]\n",
    "        for img_id in partition[\"train\"] + partition[\"test\"]\n",
    "    }\n",
    "\n",
    "    labels_to_use = labels_subset\n",
    "else:\n",
    "    partition = {\"train\": train_ids, \"test\": test_ids}\n",
    "    labels_to_use = labels_global\n",
    "\n",
    "print(f\"Training IDs: {len(partition['train'])}\")\n",
    "print(f\"Test IDs: {len(partition['test'])}\")\n",
    "print(f\"Total labels used: {len(labels_to_use)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cabf2fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"batch_size\": BATCH_SIZE, \"shuffle\": True, \"num_workers\": N_WORKERS}\n",
    "\n",
    "model = timm.create_model(\"efficientnetv2_m\", pretrained=False)\n",
    "model_config = timm.data.resolve_model_data_config(model)\n",
    "transform = timm.data.create_transform(**model_config)\n",
    "model.to(DEVICE)\n",
    "\n",
    "train_set = LandmarkDataset(partition[\"train\"], labels_to_use, directory=TRAIN_DIR, transform=transform)\n",
    "test_set = LandmarkDataset(partition[\"test\"], labels_to_use, directory=TEST_DIR, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, **params)\n",
    "test_loader = DataLoader(test_set, **params)\n",
    "\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63a617",
   "metadata": {},
   "source": [
    "##### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8209c1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]: 100%|██████████| 7/7 [00:35<00:00,  5.10s/it, accuracy=0, loss=6.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.9224, Train Acc: 0.00%\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\Admin\\PycharmProjects\\kaggle-projects\\src\\common\\datasets.py\", line 49, in __getitem__\n    feature = Image.open(path).convert(self.colorspace)\n              ^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\PycharmProjects\\kaggle-projects\\.venv\\Lib\\site-packages\\PIL\\Image.py\", line 3513, in open\n    fp = builtins.open(filename, \"rb\")\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'D:/Datasets/landmark-recognition-2021/test\\\\6/5/9/659b31d2915a01b4.jpg'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Admin\\PycharmProjects\\kaggle-projects\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\PycharmProjects\\kaggle-projects\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\PycharmProjects\\kaggle-projects\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"C:\\Users\\Admin\\PycharmProjects\\kaggle-projects\\src\\common\\datasets.py\", line 51, in __getitem__\n    raise RuntimeError(f\"Could not load image {path}: {e}\")\nRuntimeError: Could not load image D:/Datasets/landmark-recognition-2021/test\\6/5/9/659b31d2915a01b4.jpg: [Errno 2] No such file or directory: 'D:/Datasets/landmark-recognition-2021/test\\\\6/5/9/659b31d2915a01b4.jpg'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m val_total = \u001b[32m0\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\PycharmProjects\\kaggle-projects\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\PycharmProjects\\kaggle-projects\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1465\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1463\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1464\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._task_info[idx]\n\u001b[32m-> \u001b[39m\u001b[32m1465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\PycharmProjects\\kaggle-projects\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1489\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\PycharmProjects\\kaggle-projects\\.venv\\Lib\\site-packages\\torch\\_utils.py:715\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    712\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    714\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mRuntimeError\u001b[39m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\Admin\\PycharmProjects\\kaggle-projects\\src\\common\\datasets.py\", line 49, in __getitem__\n    feature = Image.open(path).convert(self.colorspace)\n              ^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\PycharmProjects\\kaggle-projects\\.venv\\Lib\\site-packages\\PIL\\Image.py\", line 3513, in open\n    fp = builtins.open(filename, \"rb\")\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'D:/Datasets/landmark-recognition-2021/test\\\\6/5/9/659b31d2915a01b4.jpg'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Admin\\PycharmProjects\\kaggle-projects\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\PycharmProjects\\kaggle-projects\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\PycharmProjects\\kaggle-projects\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"C:\\Users\\Admin\\PycharmProjects\\kaggle-projects\\src\\common\\datasets.py\", line 51, in __getitem__\n    raise RuntimeError(f\"Could not load image {path}: {e}\")\nRuntimeError: Could not load image D:/Datasets/landmark-recognition-2021/test\\6/5/9/659b31d2915a01b4.jpg: [Errno 2] No such file or directory: 'D:/Datasets/landmark-recognition-2021/test\\\\6/5/9/659b31d2915a01b4.jpg'\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        loop.set_postfix(loss=running_loss / total, accuracy=100.0 * correct / total)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    print(f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss /= val_total\n",
    "    val_acc = 100.0 * val_correct / val_total\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
