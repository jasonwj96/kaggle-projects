{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bb6ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from timm.data import resolve_model_data_config, create_transform\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from common.datasets import LandmarkDataset\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5259e073",
   "metadata": {},
   "source": [
    "##### Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bf7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATASET_PATH = \"D:/Datasets/landmark-recognition-2021\"\n",
    "TRAIN_DIR = f\"{DATASET_PATH}/train\"\n",
    "TEST_DIR = f\"{DATASET_PATH}/train\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba747168",
   "metadata": {},
   "source": [
    "##### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "N_WORKERS = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ec305",
   "metadata": {},
   "source": [
    "##### Training IDs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a40f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{DATASET_PATH}/train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca7c871",
   "metadata": {},
   "source": [
    "##### Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7209481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # for multi-GPU setups\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0506fa83",
   "metadata": {},
   "source": [
    "##### Training/Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d159394",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = len(train_df[\"landmark_id\"].value_counts())\n",
    "\n",
    "X = train_df.drop(\"landmark_id\", axis=1)\n",
    "y = train_df[\"landmark_id\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d34708",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"efficientnetv2_s\", num_classes=N_CLASSES).to(DEVICE)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transform = timm.data.create_transform(**data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2755a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_subset = True\n",
    "\n",
    "train_dataset = LandmarkDataset(X_train, y_train, transform=transform, directory=TRAIN_DIR)\n",
    "test_dataset = LandmarkDataset(X_test, y_test, transform=transform, directory=TRAIN_DIR)\n",
    "\n",
    "if use_subset:\n",
    "    train_indices = np.random.choice(len(train_dataset), 64, replace=False)\n",
    "    test_indices = np.random.choice(len(test_dataset), 64, replace=False)\n",
    "\n",
    "    train_dataset = Subset(train_dataset, train_indices)\n",
    "    test_dataset = Subset(test_dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=N_WORKERS, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=N_WORKERS, shuffle=True, pin_memory=True)\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63a617",
   "metadata": {},
   "source": [
    "##### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db36b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e708eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    train_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for images, labels in train_bar:\n",
    "        images = images.to(DEVICE, dtype=torch.float)\n",
    "        labels = labels.to(DEVICE, dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_bar.set_postfix({\n",
    "            \"loss\": f\"{running_loss/total:.4f}\",\n",
    "            \"acc\": f\"{100*correct/total:.2f}%\"\n",
    "        })\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = 100.0 * correct / total\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    val_bar = tqdm(test_loader, desc=\"Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_bar:\n",
    "            images = images.to(DEVICE, dtype=torch.float)\n",
    "            labels = labels.to(DEVICE, dtype=torch.long)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            val_bar.set_postfix({\n",
    "                \"val_loss\": f\"{val_loss/total:.4f}\",\n",
    "                \"val_acc\": f\"{100*correct/total:.2f}%\"\n",
    "            })\n",
    "\n",
    "    val_loss = val_loss / len(test_loader.dataset)\n",
    "    val_acc = 100.0 * correct / total\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
